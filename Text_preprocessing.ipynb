{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cornell Movie--Dialogs Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'cornell_movie_dialogs'\n",
    "table_name = 'dialogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = f\"postgresql://{postgres_user}:{postgres_pw}@{postgres_host}:{postgres_port}/{postgres_db}\"\n",
    "\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {table_name};\", con=engine)\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>dialogs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            dialogs\n",
       "0      0  Can we make this quick?  Roxanne Korrine and A...\n",
       "1      1  Well, I thought we'd start with pronunciation,...\n",
       "2      2  Not the hacking and gagging and spitting part....\n",
       "3      3  Okay... then how 'bout we try out some French ...\n",
       "4      4  You're asking me out.  That's so cute. What's ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "nlp.max_length = 20000000\n",
    "\n",
    "dialogs = \" \".join(df.dialogs)\n",
    "dialogs = ' '.join(dialogs.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs_doc = nlp(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dialogs_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 4189489 tokens long\n",
      "The first three tokens are 'Can we make'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Explore the objects that you've built.\n",
    "print(f\"The dialogs_doc object is a {type(dialogs_doc)} object.\")\n",
    "print(f\"It is {len(dialogs_doc)} tokens long\")\n",
    "print(f\"The first three tokens are '{dialogs_doc[:3]}'\")\n",
    "print(f\"The type of each token is {type(dialogs_doc[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "dialogs_without_stopwords = [token for token in dialogs_doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogs word freqency: [('know', 21478), ('like', 13765), ('got', 12663), ('want', 10800), ('think', 10427), ('going', 8770), ('right', 8710), ('>', 7669), ('Oh', 7516), ('time', 6452)]\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently words appear in the text\n",
    "def word_frequencies(text):\n",
    "    \n",
    "    # Build a list of words\n",
    "    # Strip out punctuation\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a `Counter` object containing word counts\n",
    "    return Counter(words)\n",
    "\n",
    "# Instantiate your list of the most common words\n",
    "dialogs_word_freq = word_frequencies(dialogs_without_stopwords).most_common(10)\n",
    "print(f'Dialogs word freqency: {dialogs_word_freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogs lemma frequency: [('know', 24745), ('go', 16867), ('like', 15621), ('get', 15302), ('think', 15044), ('want', 13985), ('come', 11007), ('tell', 10487), ('right', 10103), ('look', 8806)]\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently each lemma appears in the text\n",
    "def lemma_frequencies(text):\n",
    "    \n",
    "    # Build a list of lemmas\n",
    "    # Strip out punctuation\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a `Counter` object containing lemma counts\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate your list of most common lemmas\n",
    "dialogs_lemma_freq = lemma_frequencies(dialogs_without_stopwords).most_common(10)\n",
    "print(f'Dialogs lemma frequency: {dialogs_lemma_freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dialogs doc has 478611 sentences.\n",
      "Here is an example: Well, I thought we'd start with pronunciation, if that's okay with you.\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of sentences\n",
    "sentences = list(dialogs_doc.sents)\n",
    "print(\"The dialogs doc has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[3]\n",
    "print(f\"Here is an example: {example_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 words in this sentence, and 13 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "# Look at some metrics around this sentence\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print(f\"There are {len(example_words)} words in this sentence, and {len(unique_words)} of them are unique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter US Airline Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'twitter_sentiment'\n",
    "table_name = 'twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = f\"postgresql://{postgres_user}:{postgres_pw}@{postgres_host}:{postgres_port}/{postgres_db}\"\n",
    "\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {table_name};\", con=engine)\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>None</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>None</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>None</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>None</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>None</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>None</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>None</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>None</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>None</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0      0  570306133677760513           neutral                        1.0000   \n",
       "1      1  570301130888122368          positive                        0.3486   \n",
       "2      2  570301083672813571           neutral                        0.6837   \n",
       "3      3  570301031407624196          negative                        1.0000   \n",
       "4      4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0           None                        NaN  Virgin America   \n",
       "1           None                     0.0000  Virgin America   \n",
       "2           None                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                   None     cairdin                None              0   \n",
       "1                   None    jnardino                None              0   \n",
       "2                   None  yvonnalynn                None              0   \n",
       "3                   None    jnardino                None              0   \n",
       "4                   None    jnardino                None              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.        None   \n",
       "1  @VirginAmerica plus you've added commercials t...        None   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...        None   \n",
       "3  @VirginAmerica it's really aggressive to blast...        None   \n",
       "4  @VirginAmerica and it's a really big bad thing...        None   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800           None  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800           None  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800           None  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800           None  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica What @dhepburn said.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = \" \".join(df.text)\n",
    "tweets = \" \".join(tweets.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_doc = nlp(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweets_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 305106 tokens long\n",
      "The first three tokens are '@VirginAmerica What @dhepburn'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"The tweets_doc object is a {type(tweets_doc)} object.\")\n",
    "print(f\"It is {len(tweets_doc)} tokens long\")\n",
    "print(f\"The first three tokens are '{tweets_doc[:3]}'\")\n",
    "print(f\"The type of each token is {type(tweets_doc[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "tweets_without_stopwords = [token for token in tweets_doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word freqency: [('@united', 3733), ('flight', 3178), ('@AmericanAir', 2904), ('@USAirways', 2893), ('@SouthwestAir', 2390), ('@JetBlue', 2176), ('Cancelled', 1065), ('service', 928), ('time', 770), ('Flight', 740)]\n"
     ]
    }
   ],
   "source": [
    "tweets_word_freq = word_frequencies(tweets_without_stopwords).most_common(10)\n",
    "print(f'Word freqency: {tweets_word_freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma frequency: [('flight', 3981), ('@USAirways', 2579), ('@unite', 2445), ('@AmericanAir', 2382), ('@JetBlue', 1924), ('thank', 1634), ('@SouthwestAir', 1593), ('@united', 1333), ('hour', 1135), ('delay', 975)]\n"
     ]
    }
   ],
   "source": [
    "tweets_lemma_freq = lemma_frequencies(tweets_without_stopwords).most_common(10)\n",
    "print(f'Lemma frequency: {tweets_lemma_freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweets doc has 22687 sentences.\n",
      "Here is an example: @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;\n"
     ]
    }
   ],
   "source": [
    "sentences = list(tweets_doc.sents)\n",
    "print(\"The tweets doc has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[3]\n",
    "print(f\"Here is an example: {example_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 words in this sentence, and 14 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print(f\"There are {len(example_words)} words in this sentence, and {len(unique_words)} of them are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
