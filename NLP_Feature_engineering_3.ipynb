{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/abilenky/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
    "    # recognize: the double dash --. Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take some time.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of stop words and punctuation,\n",
    "# and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct \n",
    "                                and not token.is_stop and]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
       "      <td>0.156718</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>-0.233711</td>\n",
       "      <td>-0.071123</td>\n",
       "      <td>-0.053649</td>\n",
       "      <td>0.253381</td>\n",
       "      <td>-0.154396</td>\n",
       "      <td>0.329343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092105</td>\n",
       "      <td>-0.133990</td>\n",
       "      <td>0.008770</td>\n",
       "      <td>0.052168</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>0.158226</td>\n",
       "      <td>0.440264</td>\n",
       "      <td>0.106349</td>\n",
       "      <td>-0.103918</td>\n",
       "      <td>0.050352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.032045</td>\n",
       "      <td>-0.114876</td>\n",
       "      <td>-0.023441</td>\n",
       "      <td>-0.009155</td>\n",
       "      <td>0.127409</td>\n",
       "      <td>-0.075661</td>\n",
       "      <td>0.146582</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050087</td>\n",
       "      <td>-0.067237</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>0.022497</td>\n",
       "      <td>0.119705</td>\n",
       "      <td>0.084655</td>\n",
       "      <td>0.194639</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>-0.059510</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[remarkable, Alice, think, way, hear, Rabbit]</td>\n",
       "      <td>0.336701</td>\n",
       "      <td>0.042047</td>\n",
       "      <td>-0.402422</td>\n",
       "      <td>-0.196097</td>\n",
       "      <td>-0.074489</td>\n",
       "      <td>0.520906</td>\n",
       "      <td>-0.262239</td>\n",
       "      <td>0.529839</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145754</td>\n",
       "      <td>-0.194869</td>\n",
       "      <td>-0.018468</td>\n",
       "      <td>0.137644</td>\n",
       "      <td>0.418958</td>\n",
       "      <td>0.305857</td>\n",
       "      <td>0.671031</td>\n",
       "      <td>0.191104</td>\n",
       "      <td>-0.254096</td>\n",
       "      <td>0.113918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>0.083552</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>-0.129789</td>\n",
       "      <td>-0.042346</td>\n",
       "      <td>-0.028135</td>\n",
       "      <td>0.141970</td>\n",
       "      <td>-0.080866</td>\n",
       "      <td>0.170579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057806</td>\n",
       "      <td>-0.066599</td>\n",
       "      <td>-0.009402</td>\n",
       "      <td>0.016764</td>\n",
       "      <td>0.131715</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>0.226310</td>\n",
       "      <td>0.058827</td>\n",
       "      <td>-0.059215</td>\n",
       "      <td>0.034975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>0.083552</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>-0.129789</td>\n",
       "      <td>-0.042346</td>\n",
       "      <td>-0.028135</td>\n",
       "      <td>0.141970</td>\n",
       "      <td>-0.080866</td>\n",
       "      <td>0.170579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057806</td>\n",
       "      <td>-0.066599</td>\n",
       "      <td>-0.009402</td>\n",
       "      <td>0.016764</td>\n",
       "      <td>0.131715</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>0.226310</td>\n",
       "      <td>0.058827</td>\n",
       "      <td>-0.059215</td>\n",
       "      <td>0.034975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  [Alice, begin, tired, sit, sister, bank, have,...  0.156718   \n",
       "1  Carroll  [consider, mind, hot, day, feel, sleepy, stupi...  0.111200   \n",
       "2  Carroll      [remarkable, Alice, think, way, hear, Rabbit]  0.336701   \n",
       "3  Carroll                                         [oh, dear]  0.083552   \n",
       "4  Carroll                                         [oh, dear]  0.083552   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0  0.002684 -0.233711 -0.071123 -0.053649  0.253381 -0.154396  0.329343  ...   \n",
       "1  0.032045 -0.114876 -0.023441 -0.009155  0.127409 -0.075661  0.146582  ...   \n",
       "2  0.042047 -0.402422 -0.196097 -0.074489  0.520906 -0.262239  0.529839  ...   \n",
       "3  0.007355 -0.129789 -0.042346 -0.028135  0.141970 -0.080866  0.170579  ...   \n",
       "4  0.007355 -0.129789 -0.042346 -0.028135  0.141970 -0.080866  0.170579  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0 -0.092105 -0.133990  0.008770  0.052168  0.226100  0.158226  0.440264   \n",
       "1 -0.050087 -0.067237 -0.021828  0.022497  0.119705  0.084655  0.194639   \n",
       "2 -0.145754 -0.194869 -0.018468  0.137644  0.418958  0.305857  0.671031   \n",
       "3 -0.057806 -0.066599 -0.009402  0.016764  0.131715  0.092017  0.226310   \n",
       "4 -0.057806 -0.066599 -0.009402  0.016764  0.131715  0.092017  0.226310   \n",
       "\n",
       "         97        98        99  \n",
       "0  0.106349 -0.103918  0.050352  \n",
       "1  0.050525 -0.059510  0.041016  \n",
       "2  0.191104 -0.254096  0.113918  \n",
       "3  0.058827 -0.059215  0.034975  \n",
       "4  0.058827 -0.059215  0.034975  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train word2vec on the sentences\n",
    "size = 100\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=8,\n",
    "    sg=0,\n",
    "    sample=50,\n",
    "    size=size,\n",
    "    hs=1\n",
    ")\n",
    "\n",
    "word2vec_arr = np.zeros((sentences.shape[0],size))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "sentences.dropna(inplace=True)\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.9002624671916011\n",
      "\n",
      "Test set score: 0.9015748031496063\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9991251093613298\n",
      "\n",
      "Test set score: 0.8989501312335958\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.9886264216972879\n",
      "\n",
      "Test set score: 0.9068241469816273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences['author']\n",
    "X = np.array(sentences.drop(['text','author'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8981427390465692"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(lr, X, Y, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
